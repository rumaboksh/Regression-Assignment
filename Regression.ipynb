{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPYS2hmjwiJR980O8HUwJYQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Theoretical"],"metadata":{"id":"VPnHUGc8gyDV"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"3MHreQmGgwr2"},"outputs":[],"source":["### Q.1)   What does R-squared represent in a regression model?\n","\n","ans) R-squared (R²) represents the proportion of variance in the dependent variable that is explained by the independent variable(s) in a regression model. Here's the precise explanation:\n","\n","Mathematical definition:\n","R² = 1 - (SSR/SST)\n","where:\n","- SSR = Sum of Squared Residuals (unexplained variation)\n","- SST = Total Sum of Squares (total variation)\n","\n","In simpler terms:\n","- R² ranges from 0 to 1 (or 0% to 100%)\n","- An R² of 0.7 means 70% of the variation in your dependent variable is explained by your model\n","- The remaining 30% is due to other factors not included in the model\n","\n","Key points to understand:\n","1. A higher R² doesn't necessarily mean a better model\n","2. Adding more variables almost always increases R² (which is why adjusted R² is often preferred)\n","3. R² doesn't indicate whether:\n","   - The coefficients make sense\n","   - The variables have a causal relationship\n","   - The model meets regression assumptions\n","\n"]},{"cell_type":"code","source":["### Q.2) What are the assumptions of linear regression?\n","\n","ans) 1. Linearity: The relationship between independent and dependent variables is linear.\n","2. Independence: Observations are independent of each other.\n","3. Homoscedasticity: Constant variance of residuals.\n","4. Normality: Residuals are normally distributed.\n","5. No multicollinearity among predictors."],"metadata":{"id":"F30ONwxzhN-O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.3) What is the difference between R-squared and Adjusted R-squared?\n","\n","ans)  1. R-squared: Indicates the proportion of variance explained by the model but doesn't account for the number of predictors.\n","2. Adjusted R-squared: Adjusts R-squared for the number of predictors, penalizing for unnecessary complexity."],"metadata":{"id":"-l_JHRnLhzQF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.4) Why do we use Mean Squared Error (MSE)?\n","\n","ans)MSE is used as a loss/error function in statistics and machine learning for specific mathematical and practical advantages:\n","\n","Mathematical Definition:\n","MSE = (1/n) Σ(yᵢ - ŷᵢ)²\n","where:\n","- yᵢ is the actual value\n","- ŷᵢ is the predicted value\n","- n is the number of observations\n","\n","Key reasons for using MSE:\n","\n","1. Penalizes Large Errors\n","- Squaring makes larger errors disproportionately more significant than smaller ones\n","- Particularly useful when large errors are especially undesirable\n","- Example: A $1,000 error in price prediction is worse than two $500 errors\n","\n","2. Mathematical Properties\n","- Always positive (due to squaring)\n","- Differentiable, making it suitable for optimization algorithms\n","- Convex function, ensuring a global minimum exists\n","\n","3. Statistical Connection\n","- Direct relationship to variance and standard deviation\n","- When using least squares regression, minimizing MSE gives the same result as maximum likelihood estimation under normal distribution assumptions\n","\n","4. Practical Advantages\n","- Same units as squared dependent variable\n","- Easier to calculate derivatives (important for gradient descent)\n","- More stable than other metrics like Mean Absolute Error (MAE) for optimization\n","\n","However, MSE has limitations:\n","- Not in the same units as the original data\n","- Very sensitive to outliers\n","- Can be harder to interpret than MAE\n","\n","Would you like me to explain when you might choose a different error metric instead of MSE?"],"metadata":{"id":"gBAbzYCChzVM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.5) What does an Adjusted R-squared value of 0.85 indicate?\n","ans)  85% of the variance in the dependent variable is explained by the independent variables, adjusted for the number of predictors in the model."],"metadata":{"id":"Ver2irtThzZE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.6) How do we check for normality of residuals in linear regression?\n","ans)There are several methods to check for normality of residuals in linear regression. Here's a comprehensive breakdown:\n","\n","1. Visual Methods:\n","   - Q-Q (Quantile-Quantile) Plot\n","     * Plots theoretical vs. actual quantiles\n","     * Straight line indicates normal distribution\n","     * Deviations show non-normality\n","\n","   - Histogram of residuals\n","     * Should approximate a bell curve\n","     * Look for symmetry and shape\n","\n","2. Statistical Tests:\n","   - Shapiro-Wilk Test\n","     * Null hypothesis: data is normally distributed\n","     * Better for smaller samples (n < 50)\n","     * Most powerful normality test\n","\n","   - Kolmogorov-Smirnov Test\n","     * Tests against any continuous distribution\n","     * Less powerful than Shapiro-Wilk for normality\n","     * Better for larger samples\n","\n","   - Anderson-Darling Test\n","     * Similar to K-S test but gives more weight to tails\n","     * More sensitive to deviations in distribution tails\n","\n","3. Numerical Measures:\n","   - Skewness (should be close to 0)\n","   - Kurtosis (should be close to 3)\n","\n","In Python, you can implement these checks using:\n","```python\n","from scipy import stats\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Q-Q plot\n","stats.probplot(residuals, dist=\"norm\", plot=plt)\n","\n","# Histogram\n","plt.hist(residuals, bins='auto')\n","\n","# Shapiro-Wilk test\n","stat, p_value = stats.shapiro(residuals)\n","```\n"],"metadata":{"id":"jMVh0Znkhzcs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.7) What is multicollinearity, and how does it impact regression?\n","ans)  Multicollinearity refers to high correlations between independent variables in a regression model. Let me break down its definition, detection, and impacts precisely:\n","\n","Definition:\n","- Occurs when two or more independent variables are highly correlated\n","- Perfect multicollinearity: exact linear relationship (correlation = 1 or -1)\n","- Near multicollinearity: strong but not perfect correlation\n","\n","Detection Methods:\n","1. Variance Inflation Factor (VIF)\n","   - VIF > 5 indicates potential problem\n","   - VIF > 10 indicates serious multicollinearity\n","\n","2. Correlation Matrix\n","   - Look for correlations > 0.8 between predictors\n","\n","Impacts on Regression:\n","1. Coefficient Estimates\n","   - Become unstable\n","   - Standard errors increase\n","   - Coefficients may have wrong signs\n","   - Individual effects harder to isolate\n","\n","2. Statistical Inference\n","   - Reduced t-statistics\n","   - Wider confidence intervals\n","   - May fail to reject null hypothesis when you should\n","\n","3. Model Interpretation\n","   - Difficult to determine individual variable importance\n","   - R² remains unaffected\n","   - Predictions still valid if correlation pattern remains same\n","\n","Solutions:\n","1. Remove one of the correlated variables\n","2. Create interaction terms or combined features\n","3. Use regularization (Ridge, Lasso)\n","4. Principal Component Analysis (PCA)\n","5. Collect more data if possible\n"],"metadata":{"id":"Qcmf9R4thzfk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.8) What is Mean Absolute Error (MAE)?\n","\n","ans) MAE measures the average absolute difference between predicted and observed values, providing an easily interpretable error metric.\n","      Formula:\n","                 mae = (1 / n) * sum(abs(y_actual[i] - y_predicted[i]) for i in range(n))\n","\n"],"metadata":{"id":"mr7h5MKuhzi0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.9)What are the benefits of using an ML pipeline?\n","\n","ans)   1. Automates repetitive tasks.\n","2. Ensures consistency in data preprocessing.\n","3. Simplifies the deployment process.\n","4. Facilitates hyperparameter tuning."],"metadata":{"id":"zfCyMayujPij"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.10) Why is RMSE considered more interpretable than MSE?\n","ans)   RMSE is in the same units as the dependent variable, making it easier to interpret compared to MSE, which is squared."],"metadata":{"id":"dQFlNSa1jPmk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.11) What is pickling in Python, and how is it useful in ML?\n","ans)   Pickling serializes Python objects to save them to disk. In ML, it is used to save trained models for reuse.\n","\n"],"metadata":{"id":"_adOcCe0jPp8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.12)  What does a high R-squared value mean?\n","ans)    A high R-squared indicates that a large proportion of the variance in the dependent variable is explained by the independent variables."],"metadata":{"id":"bqD3L5kajPtk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.13)  What happens if linear regression assumptions are violated?\n","ans)  1. Predictions may become unreliable.\n","2. Coefficients might be biased.\n","3. Hypothesis testing results may be invalid."],"metadata":{"id":"kEetL9ZtjPw0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.14) How can we address multicollinearity in regression?\n","ans) 1. Use techniques like Principal Component Analysis (PCA).\n","2. Drop highly correlated predictors.\n","3. Regularize the model using methods like Ridge or Lasso regression."],"metadata":{"id":"Q_HC24-NjP0N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.15) Why do we use pipelines in machine learning?\n","ans) 1. Streamlines the workflow by combining preprocessing, feature selection, and modeling steps.\n","2. Reduces the chances of data leakage.\n"],"metadata":{"id":"c4L2RcugjP4W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.16) How is Adjusted R-squared calculated?\n","\n","ans)  The Adjusted R-squared adjusts the R-squared value to account for the number of predictors in a regression model. It penalizes the addition of irrelevant predictors that do not improve the model's performance.\n","The formula for Adjusted R-squared is:\n","                                  adjusted_r2 = 1 - ((1 - r2) * (n - 1)) / (n - p - 1)\n","Where:\n","\n","r2 is the R-squared value.\n","n is the number of observations.\n","p is the number of predictors."],"metadata":{"id":"VuBTJKVNhzmc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.17)   Why is MSE sensitive to outliers?\n","ans) MSE is sensitive to outliers due to the squaring of errors in its formula. Let me break this down mathematically:\n","\n","MSE = (1/n) Σ(yᵢ - ŷᵢ)²\n","\n","Consider this example:\n","- Normal error: (10 - 8)² = 2² = 4\n","- Outlier error: (100 - 8)² = 92² = 8,464\n","\n","The squaring operation causes:\n","1. Large errors become exponentially larger\n","2. A single large outlier can dominate the entire MSE calculation\n","3. Small and moderate errors become relatively insignificant\n","\n","Let's see a concrete comparison:\n","- Dataset A (no outliers): errors of 2, 2, 2, 2, 2\n","  * MSE = (4 + 4 + 4 + 4 + 4)/5 = 4\n","\n","- Dataset B (one outlier): errors of 2, 2, 2, 2, 92\n","  * MSE = (4 + 4 + 4 + 4 + 8,464)/5 = 1,696\n","\n","Notice how one outlier increased the MSE by 424 times!\n","\n","This is why alternatives like:\n","- MAE (Mean Absolute Error)\n","- Huber Loss\n","- RMSE with trimming\n","Are often used when dealing with datasets prone to outliers.\n"],"metadata":{"id":"0X-agFfphzqs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.18) What is the role of homoscedasticity in linear regression?\n","ans) Homoscedasticity is a key assumption in linear regression that refers to the constant variance of residuals. Here's a precise explanation of its role:\n","\n","Key Aspects:\n","1. Definition\n","- The variance of residuals should be constant across all values of predicted/independent variables\n","- Mathematically: Var(εᵢ|Xᵢ) = σ² (constant) for all i\n","\n","2. Importance\n","- Ensures OLS estimators are BLUE (Best Linear Unbiased Estimators)\n","- Makes standard errors reliable\n","- Validates inference tests (t-tests, F-tests)\n","- Enables accurate confidence intervals\n","\n","3. When Violated (Heteroscedasticity)\n","- Estimators remain unbiased but lose efficiency\n","- Standard errors become incorrect\n","- Hypothesis tests become unreliable\n","- Confidence intervals are inaccurate\n","\n","4. Detection Methods\n","- Visual: Plotting residuals vs. fitted values\n","- Statistical tests:\n","   * Breusch-Pagan test\n","   * White test\n","   * Goldfeld-Quandt test\n","\n","5. Solutions if Violated\n","- Transform variables (often log transformation)\n","- Use Weighted Least Squares (WLS)\n","- Apply robust standard errors\n","- Consider different modeling approaches\n","\n","\n"],"metadata":{"id":"LBFzLtp5n5JT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.19) What is Root Mean Squared Error (RMSE)?\n","\n","ans)  RMSE is the square root of MSE, indicating the standard deviation of prediction errors.\n","         Formula:\n","                $$\n","\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n \\left( y_i - \\hat{y}_i \\right)^2}\n","$$\n"],"metadata":{"id":"gdVM0zGGn5Ub"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.20) Why is pickling considered risky?\n","\n","ans)  1. Vulnerable to code injection attacks if loading pickle files from untrusted sources.\n","2. Not cross-platform or version-independent."],"metadata":{"id":"hjEUEbM4n5oD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.21) What alternatives exist to pickling for saving ML models?\n","\n","ans)  1.Joblib for model serialization.\n","2. ONNX or PMML for platform-independent formats.\n","3. Saving model weights using frameworks like TensorFlow or PyTorch."],"metadata":{"id":"lYNZ_UAJn5yz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.22)  What is heteroscedasticity, and why is it a problem?\n","ans)  Here is the exact definition and explanation of heteroscedasticity:\n","\n","Heteroscedasticity refers to a specific statistical condition where:\n","1. The variance of the error terms in a regression model is not constant across observations\n","2. The error terms' variability differs across values of an independent variable\n","\n","Mathematical definition:\n","- In a regression model Y = β₀ + β₁X + ε\n","- Heteroscedasticity exists when Var(ε|X) ≠ σ² (variance is not constant)\n","\n","Problems it causes:\n","1. OLS estimators remain unbiased but are no longer BLUE (Best Linear Unbiased Estimators)\n","2. Standard errors are incorrect, invalidating:\n","   - t-tests\n","   - F-tests\n","   - Confidence intervals\n","3. Statistical inference becomes unreliable\n","\n","Statistical tests to detect it:\n","1. Breusch-Pagan test\n","2. White test\n","3. Goldfeld-Quandt test\n","\n","Formal solutions:\n","1. Transform dependent variable (usually log transformation)\n","2. Use Weighted Least Squares (WLS)\n","3. Use Heteroscedasticity-consistent standard errors (White's robust standard errors)"],"metadata":{"id":"y1QEc8PHoZpU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.23)  How does adding irrelevant predictors affect R-squared and Adjusted R-squared?\n","\n","ans) Here's how adding irrelevant predictors affects both R² and Adjusted R²:\n","\n","Impact on R-squared:\n","1. R² always increases or stays the same when adding predictors, even irrelevant ones\n","2. Mathematical reason: Additional variables can only explain more variation or explain none\n","3. This increase occurs even with random noise variables\n","4. The increase is typically small for truly irrelevant predictors\n","\n","Impact on Adjusted R-squared:\n","1. Adjusts for the number of predictors using the formula:\n","   Adj R² = 1 - [(1 - R²)(n-1)/(n-k-1)]\n","   where:\n","   - n = sample size\n","   - k = number of predictors\n","\n","2. Can decrease when adding irrelevant predictors because:\n","   - Penalizes for additional variables\n","   - Only increases if new variable's t-statistic > 1\n","   - Helps prevent overfitting\n","\n","Example:\n","Original model (2 relevant predictors):\n","- R² = 0.70\n","- Adj R² = 0.69\n","\n","After adding irrelevant predictor:\n","- R² = 0.71 (increases)\n","- Adj R² = 0.68 (decreases)\n","\n"],"metadata":{"id":"BLkHElMYoZzz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Practical**"],"metadata":{"id":"Db-cezQnha8n"}},{"cell_type":"code","source":["### Q.1) Write a Python script that calculates the Mean Squared Error (MSE) and Mean Absolute Error (MAE) for a multiple linear regression model using Seaborn's diamonds dataset."],"metadata":{"id":"rLe5wYJxhgkX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans)   import seaborn as sns\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import mean_squared_error, mean_absolute_error\n","\n","# Load the diamonds dataset\n","diamonds = sns.load_dataset('diamonds').dropna()\n","\n","# Select features and target\n","X = pd.get_dummies(diamonds[['carat', 'depth', 'table', 'price']], drop_first=True)\n","y = diamonds['price']\n","\n","# Split the data\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Train the model\n","model = LinearRegression()\n","model.fit(X_train, y_train)\n","\n","# Predictions\n","y_pred = model.predict(X_test)\n","\n","# Calculate MSE and MAE\n","mse = mean_squared_error(y_test, y_pred)\n","mae = mean_absolute_error(y_test, y_pred)\n","\n","print(\"MSE:\", mse)\n","print(\"MAE:\", mae)\n"],"metadata":{"id":"1ZgR_GuLiIKh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.2) Write a Python script to calculate and print Mean Squared Error (MSE), Mean Absolute Error (MAE), and Root Mean Squared Error (RMSE) for a linear regression model."],"metadata":{"id":"hmx8ylCEidKX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans)  import numpy as np\n","\n","# RMSE Calculation\n","rmse = np.sqrt(mse)\n","\n","print(\"MSE:\", mse)\n","print(\"MAE:\", mae)\n","print(\"RMSE:\", rmse)\n"],"metadata":{"id":"jgcbxrfbigpf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.3) Write a Python script to check if the assumptions of linear regression are met. Use a scatter plot to check linearity, residuals plot for homoscedasticity, and correlation matrix for multicollinearity."],"metadata":{"id":"fmKzowTOipbH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans)  import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Linearity Check\n","sns.scatterplot(x=y_test, y=y_pred)\n","plt.title(\"Linearity Check\")\n","plt.xlabel(\"Actual Prices\")\n","plt.ylabel(\"Predicted Prices\")\n","plt.show()\n","\n","# Residuals for Homoscedasticity\n","residuals = y_test - y_pred\n","sns.scatterplot(x=y_pred, y=residuals)\n","plt.axhline(0, color='red', linestyle='--')\n","plt.title(\"Residuals for Homoscedasticity\")\n","plt.show()\n","\n","# Multicollinearity Check (Correlation Matrix)\n","corr_matrix = X.corr()\n","sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\")\n","plt.title(\"Correlation Matrix\")\n","plt.show()\n"],"metadata":{"id":"He3L3a3VivTO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.4)  Create a machine learning pipeline that standardizes the features, fits a linear regression model, and evaluates the model's R-squared score."],"metadata":{"id":"oTeSrlb9i4YZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans)  from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import r2_score\n","\n","# Create the pipeline\n","pipeline = Pipeline([\n","    ('scaler', StandardScaler()),\n","    ('regressor', LinearRegression())\n","])\n","\n","# Fit the pipeline\n","pipeline.fit(X_train, y_train)\n","\n","# Evaluate the model\n","y_pred_pipeline = pipeline.predict(X_test)\n","r2 = r2_score(y_test, y_pred_pipeline)\n","print(\"R-squared Score:\", r2)\n"],"metadata":{"id":"HEKInCtTi_NX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.5)  Implement a simple linear regression model on a dataset and print the model's coefficients, intercept, and R-squared score."],"metadata":{"id":"QiqYTTRtjFdn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans)   # Fit a simple linear regression model\n","simple_model = LinearRegression()\n","simple_model.fit(X_train[['carat']], y_train)  # Using 'carat' as a single feature\n","y_pred_simple = simple_model.predict(X_test[['carat']])\n","\n","# Print coefficients, intercept, and R-squared score\n","print(\"Coefficient:\", simple_model.coef_[0])\n","print(\"Intercept:\", simple_model.intercept_)\n","print(\"R-squared Score:\", r2_score(y_test, y_pred_simple))\n"],"metadata":{"id":"MRb2kmKfjNo3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.6)  Fit a simple linear regression model to the tips dataset and print the slope and intercept of the regression line."],"metadata":{"id":"M8U-Sl7-jS4A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans)   # Load tips dataset\n","tips = sns.load_dataset('tips')\n","\n","# Select features and target\n","X_tips = tips[['total_bill']]\n","y_tips = tips['tip']\n","\n","# Split the data\n","X_train_tips, X_test_tips, y_train_tips, y_test_tips = train_test_split(X_tips, y_tips, test_size=0.2, random_state=42)\n","\n","# Train the model\n","tips_model = LinearRegression()\n","tips_model.fit(X_train_tips, y_train_tips)\n","\n","# Print slope and intercept\n","print(\"Slope (Coefficient):\", tips_model.coef_[0])\n","print(\"Intercept:\", tips_model.intercept_)\n"],"metadata":{"id":"U_XyXKA4jY-v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.7)  Write a Python script that fits a linear regression model to a synthetic dataset with one feature. Use the model to predict new values and plot the data points along with the regression line."],"metadata":{"id":"Zw6a_Qwjjq3n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans)    import numpy as np\n","\n","# Generate synthetic data\n","X_synthetic = np.random.rand(100, 1) * 10\n","y_synthetic = 3 * X_synthetic + np.random.randn(100, 1) * 2\n","\n","# Train-test split\n","X_train_syn, X_test_syn, y_train_syn, y_test_syn = train_test_split(X_synthetic, y_synthetic, test_size=0.2, random_state=42)\n","\n","# Train the model\n","synthetic_model = LinearRegression()\n","synthetic_model.fit(X_train_syn, y_train_syn)\n","\n","# Predictions\n","y_pred_syn = synthetic_model.predict(X_test_syn)\n","\n","# Plot the data points and regression line\n","plt.scatter(X_synthetic, y_synthetic, color='blue', label='Data Points')\n","plt.plot(X_test_syn, y_pred_syn, color='red', label='Regression Line')\n","plt.legend()\n","plt.show()\n"],"metadata":{"id":"Qrx45smrjw5f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.8)  Write a Python script that pickles a trained linear regression model and saves it to a file."],"metadata":{"id":"e1Yw3yEIj2tQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans) import pickle\n","\n","# Save the model to a file\n","with open('linear_model.pkl', 'wb') as f:\n","    pickle.dump(model, f)\n","\n","print(\"Model saved to 'linear_model.pkl'\")\n"],"metadata":{"id":"ESvMHYvBj3JY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.9)  Write a Python script that fits a polynomial regression model (degree 2) to a dataset and plots the curve."],"metadata":{"id":"JfHXSscQj3So"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans)  from sklearn.preprocessing import PolynomialFeatures\n","from sklearn.linear_model import LinearRegression\n","\n","# Generate synthetic data\n","X_poly = np.random.rand(100, 1) * 10\n","y_poly = 3 * (X_poly ** 2) + 2 * X_poly + np.random.randn(100, 1) * 10\n","\n","# Polynomial features\n","poly_features = PolynomialFeatures(degree=2)\n","X_poly_transformed = poly_features.fit_transform(X_poly)\n","\n","# Train the model\n","poly_model = LinearRegression()\n","poly_model.fit(X_poly_transformed, y_poly)\n","\n","# Predictions\n","y_pred_poly = poly_model.predict(X_poly_transformed)\n","\n","# Plot\n","plt.scatter(X_poly, y_poly, color='blue', label='Data Points')\n","plt.plot(X_poly, y_pred_poly, color='red', label='Polynomial Regression Curve')\n","plt.legend()\n","plt.show()\n"],"metadata":{"id":"w6OVVvTvj3Xw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.10)  Generate synthetic data for simple linear regression (random values for X and y), fit a linear regression model, and print the coefficient and intercept."],"metadata":{"id":"oV2avu_5kTNx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans) # Generate synthetic data\n","X_simple = np.random.rand(100, 1) * 10\n","y_simple = 5 * X_simple + np.random.randn(100, 1) * 5\n","\n","# Train the model\n","simple_model = LinearRegression()\n","simple_model.fit(X_simple, y_simple)\n","\n","# Print coefficient and intercept\n","print(\"Coefficient:\", simple_model.coef_[0])\n","print(\"Intercept:\", simple_model.intercept_)\n"],"metadata":{"id":"DpTLiwwpj3cw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.11)  Write a Python script that fits a polynomial regression model (degree 3) to a synthetic dataset and plots the curve."],"metadata":{"id":"OwRAo_0HkXX_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans) # Polynomial features (degree 3)\n","poly_features_3 = PolynomialFeatures(degree=3)\n","X_poly_3 = poly_features_3.fit_transform(X_poly)\n","\n","# Train the model\n","poly_model_3 = LinearRegression()\n","poly_model_3.fit(X_poly_3, y_poly)\n","\n","# Predictions\n","y_pred_poly_3 = poly_model_3.predict(X_poly_3)\n","\n","# Plot\n","plt.scatter(X_poly, y_poly, color='blue', label='Data Points')\n","plt.plot(X_poly, y_pred_poly_3, color='green', label='Degree 3 Polynomial Regression Curve')\n","plt.legend()\n","plt.show()\n"],"metadata":{"id":"FGjbQi0FkZM5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.12) Write a Python script that fits a simple linear regression model with two features and prints the coefficients, intercept, and R-squared score."],"metadata":{"id":"vrGruSNNkZIn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans) # Select two features for linear regression\n","X_two_features = diamonds[['carat', 'depth']]\n","y_two_features = diamonds['price']\n","\n","# Train-test split\n","X_train_two, X_test_two, y_train_two, y_test_two = train_test_split(X_two_features, y_two_features, test_size=0.2, random_state=42)\n","\n","# Train the model\n","two_feature_model = LinearRegression()\n","two_feature_model.fit(X_train_two, y_train_two)\n","\n","# Predictions\n","y_pred_two = two_feature_model.predict(X_test_two)\n","\n","# Print coefficients, intercept, and R-squared score\n","print(\"Coefficients:\", two_feature_model.coef_)\n","print(\"Intercept:\", two_feature_model.intercept_)\n","print(\"R-squared Score:\", r2_score(y_test_two, y_pred_two))\n"],"metadata":{"id":"0_PGx6tjkZFG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.13)  Write a Python script that generates a synthetic dataset, fits a linear regression model, and calculates MSE, MAE, and RMSE."],"metadata":{"id":"n2_o0jDUlmOu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans)   # Generate synthetic data\n","X_syn = np.random.rand(100, 1) * 10\n","y_syn = 4 * X_syn + np.random.randn(100, 1) * 5\n","\n","# Train-test split\n","X_train_syn, X_test_syn, y_train_syn, y_test_syn = train_test_split(X_syn, y_syn, test_size=0.2, random_state=42)\n","\n","# Train the model\n","synthetic_model = LinearRegression()\n","synthetic_model.fit(X_train_syn, y_train_syn)\n","\n","# Predictions\n","y_pred_syn = synthetic_model.predict(X_test_syn)\n","\n","# Calculate MSE, MAE, RMSE\n","mse_syn = mean_squared_error(y_test_syn, y_pred_syn)\n","mae_syn = mean_absolute_error(y_test_syn, y_pred_syn)\n","rmse_syn = np.sqrt(mse_syn)\n","\n","print(\"MSE:\", mse_syn)\n","print(\"MAE:\", mae_syn)\n","print(\"RMSE:\", rmse_syn)\n"],"metadata":{"id":"cJs45xEVls03"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.14) Write a Python script that uses the Variance Inflation Factor (VIF) to check for multicollinearity in a dataset with multiple features.\n","\n"],"metadata":{"id":"SGYIX77XkY5v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans)  from statsmodels.stats.outliers_influence import variance_inflation_factor\n","\n","# Calculate VIF for each feature\n","X_multi = diamonds[['carat', 'depth', 'table', 'x', 'y', 'z']]\n","vif_data = pd.DataFrame()\n","vif_data['Feature'] = X_multi.columns\n","vif_data['VIF'] = [variance_inflation_factor(X_multi.values, i) for i in range(X_multi.shape[1])]\n","\n","print(vif_data)\n"],"metadata":{"id":"V08ve_Z4kY1_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.15) Write a Python script that generates synthetic data for a polynomial relationship (degree 4), fits a polynomial regression model, and plots the regression curve."],"metadata":{"id":"dahiJ96zkYuu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans)  # Generate synthetic data\n","X_poly4 = np.random.rand(100, 1) * 10\n","y_poly4 = 2 * (X_poly4 ** 4) - 3 * (X_poly4 ** 3) + 4 * (X_poly4 ** 2) + 5 * X_poly4 + np.random.randn(100, 1) * 10\n","\n","# Polynomial features (degree 4)\n","poly_features_4 = PolynomialFeatures(degree=4)\n","X_poly4_transformed = poly_features_4.fit_transform(X_poly4)\n","\n","# Train the model\n","poly_model_4 = LinearRegression()\n","poly_model_4.fit(X_poly4_transformed, y_poly4)\n","\n","# Predictions\n","y_pred_poly4 = poly_model_4.predict(X_poly4_transformed)\n","\n","# Plot\n","plt.scatter(X_poly4, y_poly4, color='blue', label='Data Points')\n","plt.plot(X_poly4, y_pred_poly4, color='purple', label='Degree 4 Polynomial Regression Curve')\n","plt.legend()\n","plt.show()\n"],"metadata":{"id":"RTdpufubkYqv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.16) Write a Python script that creates a machine learning pipeline with data standardization and a multiple linear regression model, and prints the R-squared score."],"metadata":{"id":"fqyKXh1fkYjw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans) # Pipeline from previous task\n","pipeline.fit(X_train, y_train)\n","r2 = pipeline.score(X_test, y_test)\n","\n","print(\"R-squared Score:\", r2)\n"],"metadata":{"id":"7G8hICUSkYfX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.17)  Write a Python script that performs polynomial regression (degree 3) on a synthetic dataset and plots the regression curve."],"metadata":{"id":"b1r50YkfkYU_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans)  import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.preprocessing import PolynomialFeatures\n","from sklearn.linear_model import LinearRegression\n","\n","# Generate synthetic data\n","np.random.seed(42)\n","X = np.random.rand(100, 1) * 10  # Random values for X in the range 0 to 10\n","y = 2 * (X ** 3) - 3 * (X ** 2) + 4 * X + np.random.randn(100, 1) * 50  # Degree 3 polynomial relationship with noise\n","\n","# Transform the features to polynomial features (degree 3)\n","poly_features = PolynomialFeatures(degree=3)\n","X_poly = poly_features.fit_transform(X)\n","\n","# Train a polynomial regression model\n","model = LinearRegression()\n","model.fit(X_poly, y)\n","\n","# Make predictions\n","y_pred = model.predict(X_poly)\n","\n","# Plot the data and the regression curve\n","plt.scatter(X, y, color='blue', label='Data Points')\n","plt.plot(X, y_pred, color='red', label='Degree 3 Regression Curve')\n","plt.title('Polynomial Regression (Degree 3)')\n","plt.xlabel('X')\n","plt.ylabel('y')\n","plt.legend()\n","plt.show()\n"],"metadata":{"id":"b1AMXvh0kX2_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.18)  Write a Python script that performs multiple linear regression on a synthetic dataset with 5 features. Print the R-squared score and model coefficients."],"metadata":{"id":"JZpDuAN-lL33"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans)  # Generate synthetic data with 5 features\n","X_syn_multi = np.random.rand(100, 5)\n","y_syn_multi = 3 * X_syn_multi[:, 0] + 2 * X_syn_multi[:, 1] - X_syn_multi[:, 2] + np.random.randn(100)\n","\n","# Train-test split\n","X_train_multi, X_test_multi, y_train_multi, y_test_multi = train_test_split(X_syn_multi, y_syn_multi, test_size=0.2, random_state=42)\n","\n","# Train the model\n","multi_model = LinearRegression()\n","multi_model.fit(X_train_multi, y_train_multi)\n","\n","# Predictions and R-squared score\n","r2_multi = multi_model.score(X_test_multi, y_test_multi)\n","\n","print(\"R-squared Score:\", r2_multi)\n","print(\"Coefficients:\", multi_model.coef_)\n"],"metadata":{"id":"Lc3Fs6JhlLzx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.19)  Write a Python script that generates synthetic data for linear regression, fits a model, and visualizes the data points along with the regression line.\n","\n"],"metadata":{"id":"la70dCP_lLvP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans)  # Generate synthetic data\n","X_visual = np.random.rand(100, 1) * 10\n","y_visual = 5 * X_visual + np.random.randn(100, 1) * 2\n","\n","# Train the model\n","visual_model = LinearRegression()\n","visual_model.fit(X_visual, y_visual)\n","\n","# Predictions\n","y_pred_visual = visual_model.predict(X_visual)\n","\n","# Plot\n","plt.scatter(X_visual, y_visual, color='blue', label='Data Points')\n","plt.plot(X_visual, y_pred_visual, color='red', label='Regression Line')\n","plt.legend()\n","plt.show()\n"],"metadata":{"id":"zyck8lxrlLcv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.20) Create a synthetic dataset with 3 features and perform multiple linear regression. Print the model's R-squared score and coefficients.\n","\n"],"metadata":{"id":"7gJcs8LxlLE3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans)  # Generate synthetic data with 3 features\n","X_syn_3 = np.random.rand(100, 3)\n","y_syn_3 = 3 * X_syn_3[:, 0] + 2 * X_syn_3[:, 1] - X_syn_3[:, 2] + np.random.randn(100)\n","\n","# Train-test split\n","X_train_3, X_test_3, y_train_3, y_test_3 = train_test_split(X_syn_3, y_syn_3, test_size=0.2, random_state=42)\n","\n","# Train the model\n","model_3_features = LinearRegression()\n","model_3_features.fit(X_train_3, y_train_3)\n","\n","# Print R-squared and coefficients\n","r2_3 = model_3_features.score(X_test_3, y_test_3)\n","print(\"R-squared Score:\", r2_3)\n","print(\"Coefficients:\", model_3_features.coef_)\n"],"metadata":{"id":"DLTambhSm4Z2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.21)  Write a Python script to pickle a trained linear regression model, save it to a file, and load it back for prediction."],"metadata":{"id":"6OetnavCm84f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans)  # Save the model\n","with open('model.pkl', 'wb') as f:\n","    pickle.dump(model_3_features, f)\n","\n","# Load the model\n","with open('model.pkl', 'rb') as f:\n","    loaded_model = pickle.load(f)\n","\n","# Test prediction with the loaded model\n","sample_data = np.array([[1.5, 2.5, -1.0]])\n","print(\"Prediction for sample data:\", loaded_model.predict(sample_data))\n"],"metadata":{"id":"q4Qo6I3ym8n-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.22) Write a Python script to perform linear regression with categorical features using one-hot encoding. Use the Seaborn tips dataset.\n","\n"],"metadata":{"id":"HZriKQaNnIWu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans)  import pandas as pd\n","import seaborn as sns\n","from sklearn.linear_model import LinearRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import r2_score\n","\n","# Load the Seaborn 'tips' dataset\n","data = sns.load_dataset('tips')\n","\n","# One-hot encoding of categorical features\n","data_encoded = pd.get_dummies(data, columns=['sex', 'smoker', 'day', 'time'], drop_first=True)\n","\n","# Define features (X) and target (y)\n","X = data_encoded.drop('tip', axis=1)\n","y = data_encoded['tip']\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Fit a linear regression model\n","model = LinearRegression()\n","model.fit(X_train, y_train)\n","\n","# Make predictions\n","y_pred = model.predict(X_test)\n","\n","# Print R-squared score\n","print(\"R-squared Score:\", r2_score(y_test, y_pred))\n"],"metadata":{"id":"nLDKbzU9nIrm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.23) Compare Ridge Regression with Linear Regression on a synthetic dataset and print the coefficients and R-squared score.\n"],"metadata":{"id":"EacwUTONnJBG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans)   import numpy as np\n","from sklearn.linear_model import LinearRegression, Ridge\n","from sklearn.metrics import r2_score\n","from sklearn.model_selection import train_test_split\n","\n","# Generate synthetic dataset\n","np.random.seed(42)\n","X = np.random.rand(100, 1) * 10\n","y = 3 * X + 2 + np.random.randn(100, 1) * 2\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Linear Regression\n","lr_model = LinearRegression()\n","lr_model.fit(X_train, y_train)\n","y_pred_lr = lr_model.predict(X_test)\n","\n","# Ridge Regression\n","ridge_model = Ridge(alpha=1.0)\n","ridge_model.fit(X_train, y_train)\n","y_pred_ridge = ridge_model.predict(X_test)\n","\n","# Print coefficients and R-squared scores\n","print(\"Linear Regression Coefficients:\", lr_model.coef_)\n","print(\"Linear Regression R-squared Score:\", r2_score(y_test, y_pred_lr))\n","print(\"Ridge Regression Coefficients:\", ridge_model.coef_)\n","print(\"Ridge Regression R-squared Score:\", r2_score(y_test, y_pred_ridge))\n"],"metadata":{"id":"CBOvpfEUnP0u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.24) Write a Python script that uses cross-validation to evaluate a Linear Regression model on a synthetic dataset."],"metadata":{"id":"cimqrvI1nPjW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans) import numpy as np\n","from sklearn.linear_model import LinearRegression\n","from sklearn.model_selection import cross_val_score\n","\n","# Generate synthetic dataset\n","np.random.seed(42)\n","X = np.random.rand(100, 1) * 10\n","y = 5 * X + 7 + np.random.randn(100, 1) * 3\n","\n","# Initialize Linear Regression model\n","model = LinearRegression()\n","\n","# Perform cross-validation\n","scores = cross_val_score(model, X, y, cv=5, scoring='r2')\n","\n","# Print cross-validation R-squared scores\n","print(\"Cross-Validation R-squared Scores:\", scores)\n","print(\"Mean R-squared Score:\", np.mean(scores))\n"],"metadata":{"id":"b3FhRBVqnTKe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.25) Write a Python script that compares polynomial regression models of different degrees and prints the R-squared score for each.\n"],"metadata":{"id":"L28yJHaMn-4W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans) import numpy as np\n","from sklearn.preprocessing import PolynomialFeatures\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import r2_score\n","from sklearn.model_selection import train_test_split\n","\n","# Generate synthetic dataset\n","np.random.seed(42)\n","X = np.random.rand(100, 1) * 10\n","y = 2 * (X ** 3) - 3 * (X ** 2) + 4 * X + np.random.randn(100, 1) * 50\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","for degree in range(1, 6):  # Degrees 1 to 5\n","    poly_features = PolynomialFeatures(degree=degree)\n","    X_train_poly = poly_features.fit_transform(X_train)\n","    X_test_poly = poly_features.transform(X_test)\n","\n","    model = LinearRegression()\n","    model.fit(X_train_poly, y_train)\n","    y_pred = model.predict(X_test_poly)\n","\n","    print(f\"Degree {degree} R-squared Score:\", r2_score(y_test, y_pred))\n"],"metadata":{"id":"8QGS640gn-tm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.26) Write a Python script that adds interaction terms to a linear regression model and prints the coefficients."],"metadata":{"id":"nFaJwI1Wn-h-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans) import numpy as np\n","import pandas as pd\n","from sklearn.preprocessing import PolynomialFeatures\n","from sklearn.linear_model import LinearRegression\n","\n","# Generate synthetic dataset\n","np.random.seed(42)\n","X = pd.DataFrame({\n","    'feature1': np.random.rand(100) * 10,\n","    'feature2': np.random.rand(100) * 5\n","})\n","y = 3 * X['feature1'] + 2 * X['feature2'] + 1.5 * X['feature1'] * X['feature2'] + np.random.randn(100) * 2\n","\n","# Add interaction terms\n","poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n","X_interaction = poly.fit_transform(X)\n","\n","# Fit a linear regression model\n","model = LinearRegression()\n","model.fit(X_interaction, y)\n","\n","# Print coefficients\n","print(\"Coefficients:\", model.coef_)\n"],"metadata":{"id":"-C20y3O8n-W2"},"execution_count":null,"outputs":[]}]}